<!DOCTYPE html>
<html lang="en-us">

<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="The first workshop on Advanced Machine Vision for real-life and industrially relevant applications.">

<base href="https://stevenputtemans.github.io/AMV2018/">
<title>


     Program 

</title>
<link rel="canonical" href="https://stevenputtemans.github.io/AMV2018/program/">






<script type="text/javascript" src="/AMV2018/js/jquery-3.3.1.min.js"></script>


<link rel="stylesheet" href="/AMV2018/css/font-awesome.min.css">
<link rel="stylesheet" href="/AMV2018/css/nunito_sans.css">


    <link rel="stylesheet" href="/AMV2018/css/light-style.css">






<link rel="shortcut icon" href="/AMV2018/img/fav.ico">



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-120928140-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>





</head>


<body>
<div class="section" id="top">
    

    <div class="container hero  fade-in one ">
        
        <h1 class="bold-title is-1">Program</h1>
    </div>
    

    
    <div class="section  fade-in two ">
        <div class="container">
            
            <hr>
<nav class="nav nav-center">
    <span id="nav-toggle" class="nav-toggle"  onclick="document.getElementById('nav-menu').classList.toggle('is-active');">
      <span></span>
      <span></span>
      <span></span>
    </span>
    <div id="nav-menu" class="nav-left nav-menu">
      <span class="nav-item">
        <a href="/AMV2018/">Main</a>
      </span>
      <span class="nav-item">
        <a href="/AMV2018/about">About</a>
      </span>
      
      
      <span class="nav-item">
        <a href="/AMV2018/contact">Contact</a>
      </span>
    
      <span class="nav-item">
        <a href="https://stevenputtemans.github.io/AMV2018/index.xml"><i class="fa fa-rss"></i></a>
      </span>
    
    </div>
</nav>
<hr>

        </div>
        

        <div class="container markdown  fade-in two  top-pad">
            
            
            

<h2 id="invited-speakers">Invited speakers</h2>

<h3 id="1-yongsheng-gao">1) Yongsheng Gao</h3>

<p><img align="right" src="img/profile_yongsheng.jpg"></p>

<h4 id="vision-perception-for-automation-in-agriculture-and-aquaculture">Vision Perception for Automation in Agriculture and Aquaculture</h4>

<p>Environmental informatics studies new knowledge, technologies and devices for automation in agriculture and aquaculture, early detection of pest and plant disease, automatic species identification, plant phenomics, better water resource management, land environment monitoring, costal environment monitoring, marine life surveillance, etc. In this talk, he will introduce some of their work on automation in agriculture and aquaculture, faster grading and packing, species and cultivar identification, pest and disease recognition at Environmental Informatics @ Griffith, including recognition without detection, large image database retrieval (speed vs accuracy), and pose difference.</p>

<h4 id="biography">Biography</h4>

<p>Professor Yongsheng Gao is the Director of Australian Research Council (ARC) Research Hub for Driving Farming Productivity and Disease Prevention, the founding Leader of the Environmental Informatics flagship group and the Director of Computer Vision and Image Processing Research Lab at Griffith University. He served as the project leader of Biosecurity Group, National ICT Australia (ARC Centre of Excellence) from 2009 to 2011. He is a current member of College of Experts (Panel Member), Australian Research Council. As a Chief Investigator, he has been working on projects in Australia, Singapore, Germany, and China in the areas of smart farming, biosecurity, face recognition, biometrics, image retrieval, computer vision, pattern recognition, environmental informatics, and medical imaging. He was also employed as a consultant by Panasonic Singapore Laboratories Pte Ltd working on the face recognition standard in MPEG-7. His research are reported in the media in Australia and Singapore, including The Australian, The Courier Mail, The Sydney Morning Herald, and The Straits Times (Singapore).</p>

<h3 id="2-faisal-shafait">2) Faisal Shafait</h3>

<p><img align="right" src="img/profile_faisal.jpg"></p>

<h4 id="packagex-a-journey-from-deep-learning-to-hand-crafting">PackageX: A journey from Deep Learning to Hand Crafting</h4>

<p>Document Image Analysis and Recognition has served as a proven test bed for machine learning and computer vision research. Deep learning architectures like Convolutional Neural Networks and Long Short-Term Memory Networks made their mark on various document recognition problems before they got the attention of the broader research community. In this talk, I will present our work on reading package labels and how that evolved into PackageX, the last yard AI solution for managing incoming deliveries which is currently deployed across four continents. I will illustrate our journey that started with deep learning at its core but soon started heading towards classical pattern recognition techniques. In doing so, I will highlight various practical aspects that are often ignored in deep learning research but are crucial for deploying large scale systems in the real-world. I will conclude by providing some recommendations for design considerations when developing practical computer vision and machine learning systems.</p>

<h4 id="biography-1">Biography</h4>

<p>Dr. Faisal Shafait is currently working as the Director of Deep Learning Laboratory at the National Center of Artificial Intelligence, Islamabad, Pakistan as well as a Professor at School of Electrical Engineering and Computer Science, National University of Sciences and Technology (NUST), Islamabad, Pakistan. Previously, he was an Assistant Research Professor at the School of Computer Science and Software Engineering at The University of Western Australia in Perth, Australia; a Senior Researcher at the German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; and a Visiting Researcher at Google Inc., Mountain View, California. He received his PhD with the highest distinction in computer engineering from TU Kaiserslautern, Germany in 2008. His research interests include machine learning and pattern recognition with a special emphasis on applications in document image analysis. He has co-authored over 150 publications in international peer-reviewed conferences and journals in this area. He is serving as the Founding President of Pakistani Pattern Recognition Society, which is IAPR&rsquo;s official chapter in Pakistan. Besides academics, Faisal has also been actively involved in a number of startup and spin-off companies and is currently working as CTO of VisionX Technologies, LLC.</p>

<h2 id="tentative-program">Tentative program</h2>

<table>
<thead>
<tr>
<th align="left">Time</th>
<th align="left">What</th>
<th align="left">Who</th>
<th align="left">Title</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">08:30</td>
<td align="left">Conference Registration Open</td>
<td align="left">- - - -</td>
<td align="left">- - -</td>
</tr>

<tr>
<td align="left">09:00</td>
<td align="left">Welcome</td>
<td align="left">- - -</td>
<td align="left">- - -</td>
</tr>

<tr>
<td align="left">09:15</td>
<td align="left">Keynote</td>
<td align="left">YongSheng Gao</td>
<td align="left">Vision Perception for Automation in Agriculture and Aquaculture</td>
</tr>

<tr>
<td align="left">10:00</td>
<td align="left">Coffee Break</td>
<td align="left">- - -</td>
<td align="left">- - -</td>
</tr>

<tr>
<td align="left">10:30</td>
<td align="left">Oral Paper 1</td>
<td align="left">Sina Shafaei</td>
<td align="left">Integration of Driver Behavior into Emotion Recognition Systems: A Preliminary Study on Steering Wheel and Vehicle Acceleration</td>
</tr>

<tr>
<td align="left">10:55</td>
<td align="left">Oral Paper 2</td>
<td align="left">Feras Almasri</td>
<td align="left">Multimodal Sensor Fusion In Single Thermal image Super-Resolution</td>
</tr>

<tr>
<td align="left">11:20</td>
<td align="left">Oral Paper 3</td>
<td align="left">Robert Frohlich</td>
<td align="left">Simultaneous Multi-View Relative Pose Estimation and 3D Reconstruction from Planar Regions</td>
</tr>

<tr>
<td align="left">11:45</td>
<td align="left">Oral Paper 4</td>
<td align="left">Quan Kong</td>
<td align="left">Multimodal Deep Neural Networks based Ensemble Learning for X-ray Object Recognition</td>
</tr>

<tr>
<td align="left">12:10</td>
<td align="left">Lunch Break</td>
<td align="left">- - -</td>
<td align="left">- - -</td>
</tr>

<tr>
<td align="left">13:35</td>
<td align="left">Keynote</td>
<td align="left">Faisal Shafait</td>
<td align="left">PackageX: A journey from Deep Learning to Hand Crafting</td>
</tr>

<tr>
<td align="left">14:20</td>
<td align="left">Oral Paper 5</td>
<td align="left">Christian Bartz</td>
<td align="left">LoANs: Weakly Supervised Object Detection with Localizer Assessor Networks</td>
</tr>

<tr>
<td align="left">14:45</td>
<td align="left">Oral Paper 6</td>
<td align="left">Yasir Jan</td>
<td align="left">WNet: Joint multiple head detection and head pose estimation from a spectator crowd image</td>
</tr>

<tr>
<td align="left">15:10</td>
<td align="left">Coffee break + Poster session</td>
<td align="left">- - -</td>
<td align="left">- - -</td>
</tr>

<tr>
<td align="left">17:10</td>
<td align="left">Best paper award</td>
<td align="left">- - -</td>
<td align="left">- - -</td>
</tr>

<tr>
<td align="left">17:20</td>
<td align="left">Closing remarks</td>
<td align="left">- - -</td>
<td align="left">- - -</td>
</tr>
</tbody>
</table>

<p>Papers presented solely during poster session:</p>

<ul>
<li>Reaching behind Specular Highlights by Registration of Two Images of Broiler Viscera</li>
<li>Unconstrained Iris Segmentation using Convolutional Neural Networks</li>
<li>Visual Siamese Clustering for Cosmetic Product Recommendation</li>
<li>PCA-RECT: An Energy-efficient Object Detection Approach for Event Cameras</li>
<li>Anomaly Detection Using GANs for Visual Inspection in Noisy Training Data</li>
<li>Prediction based deep autoencoding model for anomaly detection</li>
<li>Markerless Augmented Advertising for Sports Videos</li>
</ul>

<p>All oral presentations will also have a poster available to foster further discussion on the topics presented.</p>

        </div>
        

        <div class="disqus">
            
        </div>

        <div class="container has-text-centered top-pad">
            <hr>
            <a href="#top">
                <i class="fa fa-arrow-up"></i>
            </a>
            <hr>
        </div>

        <div class="section" id="footer">
    <div class="container has-text-centered">
    
        AMV2018 workshop website made with the <a href="https://themes.gohugo.io/hugo-theme-introduction/">Introduction</a> theme for <a href="https://gohugo.io/">Hugo</a>.
    
    </div>
</div>

    </div>
    
</div>



<script>
    $('a[href^="https:\/\/stevenputtemans.github.io\/AMV2018\/program\/#"]').click(function (e) {
        e.preventDefault();
        var target = this.hash;
        $('html, body').animate({
            scrollTop: $(target).offset().top
        }, 500);
        return false;
    })
</script>

</body>
